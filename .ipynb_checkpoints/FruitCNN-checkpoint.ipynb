{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b4f331-51bb-41b1-aa23-5f6179f4cbf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IrisDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 90\u001b[0m\n\u001b[0;32m     83\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     84\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)),  \n\u001b[0;32m     85\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     86\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m     87\u001b[0m ])\n\u001b[0;32m     89\u001b[0m dataset_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miris\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m---> 90\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m IrisDataset(dataset_root, transform\u001b[38;5;241m=\u001b[39mtransform, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m IrisDataset(dataset_root, transform\u001b[38;5;241m=\u001b[39mtransform, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m IrisDataset(dataset_root, transform\u001b[38;5;241m=\u001b[39mtransform, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IrisDataset' is not defined"
     ]
    }
   ],
   "source": [
    "#COS 470/570: IRIS classification\n",
    "#Xin Zhang\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Dataset class\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, subset=\"train\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = ['iris-setosa', 'iris-versicolour', 'iris-virginica']\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for idx, cls in enumerate(self.classes):\n",
    "            cls_folder = os.path.join(self.root_dir, cls)\n",
    "            class_files = [os.path.join(cls_folder, img) for img in os.listdir(cls_folder)]\n",
    "            random.shuffle(class_files)  # Shuffle the files within this class\n",
    "\n",
    "            # Calculate split indices for the current class\n",
    "            if cls == \"iris-versicolour\":\n",
    "                total_items = 80\n",
    "            else:\n",
    "                total_items = len(class_files)\n",
    "            train_end = int(0.6 * total_items)\n",
    "            val_end = int(0.8 * total_items)\n",
    "\n",
    "            # Split the files based on the subset parameter\n",
    "            if subset == \"train\":\n",
    "                files_subset = class_files[:train_end]\n",
    "            elif subset == \"val\":\n",
    "                files_subset = class_files[train_end:val_end]\n",
    "            else:  # 'test' or other unspecified values default to test set\n",
    "                files_subset = class_files[val_end:total_items]\n",
    "\n",
    "            # Append current class files to the main list\n",
    "            self.files.extend(files_subset)\n",
    "            self.labels.extend([idx] * len(files_subset))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# CNN model\n",
    "class FruitCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FruitCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 120)  \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)  # 3 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x,2,2)\n",
    "        x = x.view(-1, 32 * 64 * 64)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "       \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_root = 'iris' \n",
    "train_dataset = IrisDataset(dataset_root, transform=transform, subset=\"train\")\n",
    "val_dataset = IrisDataset(dataset_root, transform=transform, subset=\"val\")\n",
    "test_dataset = IrisDataset(dataset_root, transform=transform, subset=\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = FruitCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training and Validation\n",
    "for epoch in range(200):  \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)  \n",
    "        _, predicted = torch.max(outputs, 1)  \n",
    "        total += labels.size(0)  \n",
    "        correct += (predicted == labels).sum().item() \n",
    "    epoch_loss = running_loss / total  \n",
    "    epoch_accuracy = 100 * correct / total  \n",
    "\n",
    "    print(f'Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "    if epoch % 3 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f'Epoch {epoch+1}, Validation Accuracy: {100 * correct / total}%, Avg Loss: {val_loss / total}')\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total}%, Avg Loss: {test_loss / total}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0085be-9c58-43ba-8e9c-389c38b59b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b4a24d-b02e-457b-98c9-732c8e7eb1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f392b-b033-4620-85f7-6a617cb3cd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfbc689-c002-44fb-b81b-5bf7f87b7126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
